<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Yue-Wen Fang - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Yue-Wen Fang">
<meta property="og:title" content="Yue-Wen Fang">


  <link rel="canonical" href="https://github.com/pages/yw-fang/yw-fang.github.io/_pages/includes/pub/">
  <meta property="og:url" content="https://github.com/pages/yw-fang/yw-fang.github.io/_pages/includes/pub/">











<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/ywfang_photo.jpg" class="author__avatar" alt="Yue-Wen Fang">
  </div>

  <div class="author__content">
    <h3 class="author__name">Yue-Wen Fang</h3>
    <p class="author__bio">CFM (CSIC-UPV/EHU)</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Postdoc at UPV/EHU.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Spain</li>
      
      
      
      
        <li><a href="mailto:fyuewen@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/YuewenFang"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
      
      
        <li><a href="https://www.linkedin.com/in/yuewen-fang-cfm"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
      
        <li><a href="https://github.com/yw-fang"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?hl=en&user=6NU1KPQAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0003-3674-7352"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:fyuewen@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
        <a href="https://twitter.com/YuewenFang"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i></a>
      
      
      
      
        <a href="https://www.linkedin.com/in/yuewen-fang-cfm"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/yw-fang"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?hl=en&user=6NU1KPQAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0000-0003-3674-7352"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="üìù Publications">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<h1 id="-publications">üìù Publications</h1>
<h2 id="-speech-synthesis">üéô Speech Synthesis</h2>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2019</div><img src="images/fs.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf">FastSpeech: Fast, Robust and Controllable Text to Speech</a> <br />
<strong>Yi Ren</strong>, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu</p>

    <p><a href="https://speechresearch.github.io/fastspeech/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

    <ul>
      <li>FastSpeech is the first fully parallel end-to-end speech synthesis model.</li>
      <li><strong>Academic Impact</strong>: This work is included by many famous speech synthesis open-source projects, such as <a href="https://github.com/espnet/espnet">ESPNet <img src="https://img.shields.io/github/stars/espnet/espnet?style=social" alt="" /></a>. Our work are promoted by more than 20 media and forums, such as <a href="https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ">Êú∫Âô®‰πãÂøÉ</a>„ÄÅ<a href="https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu">InfoQ</a>.</li>
      <li><strong>Industry Impact</strong>: FastSpeech has been deployed in <a href="https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911">Microsoft Azure TTS service</a> and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in <a href="https://resources.nvidia.com/events/GTC2020s21420">NVIDIA GTC2020</a>.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR 2021</div><img src="images/fs2.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2006.04558">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a> <br />
<strong>Yi Ren</strong>, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu</p>

    <p><a href="https://speechresearch.github.io/fastspeech2/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="4FA6C0AAAAAJ:LkGwnXOMwfcC"></span></strong></p>
    <ul>
      <li>This work is included by many famous speech synthesis open-source projects, such as <a href="https://github.com/PaddlePaddle/PaddleSpeech">PaddlePaddle/Parakeet <img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?style=social" alt="" /></a>, <a href="https://github.com/espnet/espnet">ESPNet <img src="https://img.shields.io/github/stars/espnet/espnet?style=social" alt="" /></a> and <a href="https://github.com/pytorch/fairseq">fairseq <img src="https://img.shields.io/github/stars/pytorch/fairseq?style=social" alt="" /></a>.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI 2022</div><img src="images/diffsinger.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2105.02446">DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism</a> <br />
Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, Feiyang Chen, Zhou Zhao</p>

    <ul>
      <li>Many <a href="https://www.bilibili.com/video/BV1be411N7JA">video demos</a> created by the <a href="https://github.com/openvpi">DiffSinger community</a> are released.</li>
      <li>
        <p>DiffSinger was introduced in <a href="https://www.bilibili.com/video/BV1uM411t7ZJ">a very popular video</a> (1600k+ views) on Bilibili!</p>
      </li>
      <li><a href="https://diffsinger.github.io/"><strong>Project</strong></a> | <a href="https://github.com/NATSpeech/NATSpeech"><img src="https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&amp;label=DiffSpeech Stars" alt="" /></a> | <a href="https://github.com/MoonInTheRiver/DiffSinger"><img src="https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&amp;label=DiffSinger Stars" alt="" /></a> | <a href="https://huggingface.co/spaces/NATSpeech/DiffSpeech"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo" alt="Hugging Face" /></a></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2021</div><img src="images/portaspeech.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2109.15166">PortaSpeech: Portable and High-Quality Generative Text-to-Speech</a> <br />
<strong>Yi Ren</strong>, Jinglin Liu, Zhou Zhao</p>

    <p><a href="https://portaspeech.github.io/"><strong>Project</strong></a> | <a href="https://github.com/NATSpeech/NATSpeech"><img src="https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&amp;label=Code+Stars" alt="" /></a> | <a href="https://huggingface.co/spaces/NATSpeech/PortaSpeech"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo" alt="Hugging Face" /></a></p>
  </div>
</div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=SbR9mpTuBn">Bag of Tricks for Unsupervised Text-to-Speech</a>, <strong>Yi Ren</strong>, Chen Zhang, Shuicheng Yan</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech</a>, Ziyue Jiang, Zhe Su, Zhou Zhao, Qian Yang, <strong>Yi Ren</strong>, Jinglin Liu, Zhenhui Ye <a href="https://github.com/Zain-Jiang/Dict-TTS"><img src="https://img.shields.io/github/stars/Zain-Jiang/Dict-TTS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech</a>, Rongjie Huang, <strong>Yi Ren</strong>, Jinglin Liu, Chenye Cui, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus</a>, Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, <strong>Yi Ren</strong>, Jinzheng He, Rongjie Huang, Jieming Zhu, Xiao Chen, Zhou Zhao, <em>(Datasets and Benchmarks Track)</em> <a href="https://github.com/M4Singer/M4Singer"><img src="https://img.shields.io/github/stars/M4Singer/M4Singer?style=social&amp;label=Dataset+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech</a>, Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, <strong>Yi Ren</strong>, <a href="https://github.com/Rongjiehuang/ProDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/ProDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="https://arxiv.org/abs/2110.07468">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</a>, Rongjie Huang, Chenye Cui, Chen Feiayng, <strong>Yi Ren</strong>, Jinglin Liu, Zhou Zhao, Baoxing Huai, Zhefeng Wang</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</a>, Zhenhui Ye, Zhou Zhao, <strong>Yi Ren</strong>, Fei Wu, <a href="https://github.com/yerfor/SyntaSpeech"><img src="https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <span style="color:red">(Oral)</span> <a href="">EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling</a>, Lichao Zhang, Zhou Zhao, <strong>Yi Ren</strong>, Liqun Deng,</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</a>, Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, <strong>Yi Ren</strong>, Zhou Zhao,  <span style="color:red">(Oral)</span>, <a href="https://github.com/Rongjiehuang/FastDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="">A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</a>, Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, <strong>Yi Ren</strong>, Tao Qin, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13066">Revisiting Over-Smoothness in Text to Speech</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13277">Learning the Beauty in Songs: Neural Singing Voice Beautifier</a>, Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, Zhiying Zhu, Zhou Zhao | <a href="https://github.com/MoonInTheRiver/NeuralSVB"><img src="https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2022</code> <a href="https://prosospeech.github.io/">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech</a>, <strong>Yi Ren</strong>, Ming Lei, Zhiying Huang,  Shiliang Zhang, Qian Chen, Zhijie Yan, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <a href="https://arxiv.org/abs/2106.09317">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</a>, Chenye Cui, <strong>Yi Ren</strong>, Jinglin Liu, Feiyang Chen, Rongjie Huang, Ming Lei and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <span style="color:red">(best student paper award candidate)</span> <a href="https://arxiv.org/abs/2106.08507">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</a>, Kexun Zhang, <strong>Yi Ren</strong>, Changliang Xu and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2021</code> <a href="https://arxiv.org/abs/2012.09547">Denoising Text to Speech with Frame-Level Noise Modeling</a>, Chen Zhang, <strong>Yi Ren</strong>, Xu Tan, Jinglin Liu, Kejun Zhang, Tao Qin, Sheng Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/denoispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2021</code> <a href="https://arxiv.org/pdf/2112.10358">Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus</a>, Rongjie Huang, Feiyang Chen, <strong>Yi Ren</strong>, Jinglin Liu, Chenye Cui, Zhou Zhao <span style="color:red">(Oral)</span></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2021</code> <a href="https://www.ijcai.org/proceedings/2021/527">FedSpeech: Federated Text-to-Speech with Continual Learning</a>, Ziyue Jiang, <strong>Yi Ren</strong>, Ming Lei and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403249">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/deepsinger/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403331">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a>, Jin Xu, Xu Tan, <strong>Yi Ren</strong>, Tao Qin, Jian Li, Sheng Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/lrspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2020</code> <a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3139.pdf">MultiSpeech: Multi-Speaker Text to Speech with Transformer</a>, Mingjian Chen, Xu Tan, <strong>Yi Ren</strong>, Jin Xu, Hao Sun, Sheng Zhao, Tao Qin | <a href="https://speechresearch.github.io/multispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2019</code> <span style="color:red">(Oral)</span> <a href="https://pdfs.semanticscholar.org/9075/a3e6271e5ef4953491488d1776527e632408.pdf">Almost Unsupervised Text to Speech and Automatic Speech Recognition</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu  | <a href="https://speechresearch.github.io/unsuper/"><strong>Project</strong></a></li>
</ul>

<h2 id="-talkingface-generation">üëÑ Talkingface Generation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=YfwMIDhPccD">GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a>, Zhenhui Ye, Ziyue Jiang, <strong>Yi Ren</strong>, Jinglin Liu, Jinzheng He, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://arxiv.org/abs/2107.06831">Parallel and High-Fidelity Text-to-Lip Generation</a>, Jinglin Liu, Zhiying Zhu, <strong>Yi Ren</strong>, Wencan Huang, Baoxing Huai, Nicholas Yuan, Zhou Zhao | <a href="https://github.com/Dianezzy/ParaLip"><img src="https://img.shields.io/github/stars/Dianezzy/ParaLip?style=social&amp;label=ParaLip Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19966">Flow-based Unconstrained Lip to Speech Generation</a>, Jinzheng He, Zhou Zhao, <strong>Yi Ren</strong>, Jinglin Liu, Baoxing Huai, Nicholas Yuan</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <a href="https://dl.acm.org/doi/10.1145/3394171.3413740">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</a>, Jinglin Liu, <strong>Yi Ren</strong>, Zhou Zhao, Chen Zhang, Baoxing Huai, Jing Yuan</li>
</ul>

<h2 id="-machine-translation">üìö Machine Translation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=UVAmFAtC5ye">TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</a>, Rongjie Huang, Jinglin Liu, Huadai Liu, <strong>Yi Ren</strong>, Lichao Zhang, Jinzheng He, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2006.07926">UWSpeech: Speech to Speech Translation for Unwritten Languages</a>, Chen Zhang, Xu Tan, <strong>Yi Ren</strong>, Tao Qin, Kejun Zhang, Tie-Yan Liu | <a href="https://speechresearch.github.io/uwspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2020</code> <a href="https://www.ijcai.org/Proceedings/2020/0534.pdf">Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation</a>, Jinglin Liu, <strong>Yi Ren</strong>, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao and Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://www.aclweb.org/anthology/2020.acl-main.350">SimulSpeech: End-to-End Simultaneous Speech to Text Translation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, Chen Zhang, Qin Tao, Zhou Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://arxiv.org/abs/2004.10454">A Study of Non-autoregressive Model for Sequence Generation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, Tie-Yan Liu</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2019</code> <a href="https://openreview.net/forum?id=S1gUsoR9YX">Multilingual Neural Machine Translation with Knowledge Distillation</a>, Xu Tan, <strong>Yi Ren</strong>, Di He, Tao Qin, Zhou Zhao, Tie-Yan Liu</li>
</ul>

<h2 id="-music-generation">üéº Music Generation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2012.05168">SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint</a>, Zhonghao Sheng, Kaitao Song, Xu Tan, <strong>Yi Ren</strong>, Wei Ye, Shikun Zhang, Tao Qin</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <span style="color:red">(Oral)</span> <a href="https://dl.acm.org/doi/10.1145/3394171.3413721">PopMAG: Pop Music Accompaniment Generation</a>, <strong>Yi Ren</strong>, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu | <a href="https://speechresearch.github.io/popmag/"><strong>Project</strong></a></li>
</ul>

<h2 id="-generative-model">üßë‚Äçüé® Generative Model</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2022</code> <a href="https://openreview.net/forum?id=PlKWVd2yBkY">Pseudo Numerical Methods for Diffusion Models on Manifolds</a>, Luping Liu, <strong>Yi Ren</strong>, Zhijie Lin, Zhou Zhao | <a href="https://github.com/luping-liu/PNDM"><img src="https://img.shields.io/github/stars/luping-liu/PNDM?style=social&amp;label=Code+Stars" alt="" /></a> | <a href="https://paperswithcode.com/sota/image-generation-on-celeba-64x64?p=pseudo-numerical-methods-for-diffusion-models-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pseudo-numerical-methods-for-diffusion-models-1/image-generation-on-celeba-64x64" alt="PWC" /></a></li>
</ul>

<h2 id="others">Others</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">Video-Guided Curriculum Learning for Spoken Video Grounding</a>, Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, <strong>Yi Ren</strong></li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=6NU1KPQAAAAJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "6NU1KPQAAAAJ");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/yw-fang/yw-fang.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
